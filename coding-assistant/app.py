import os, gc, torch, streamlit as st
from coding_agent import generate_code_response, reset_history
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

# â”€â”€ UI SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ðŸ’» Coding Assistant", page_icon="ðŸ¤–")
st.title("ðŸ’» Coding Assistant")
st.markdown("Chat, complete, or use the fine-tuned model for coding tasks.")

# â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
hf_token      = os.getenv("HF_TOKEN")
if not hf_token:
    st.sidebar.error("ðŸš¨ Set HF_TOKEN as an env var to use this app.")
    st.stop()

mode          = st.sidebar.selectbox("Mode", ["chat", "completion", "fine-tuned"])
  
# â”€â”€ Chat history â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "history" not in st.session_state:
    st.session_state.history = []

# â”€â”€ Input area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
prompt = st.text_area("Enter your prompt here", height=120)
if st.button("Submit") and prompt:
    with st.spinner("ðŸ’¡ Generatingâ€¦"):
        if mode == "fine-tuned":
            # load LoRA-fine-tuned adapter
            adapter_path = "output/qlora-deepseek/adapters"
            config       = PeftConfig.from_pretrained(adapter_path)
            base_model   = AutoModelForCausalLM.from_pretrained(
                config.base_model_name_or_path,
                device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
            )
            model        = PeftModel.from_pretrained(base_model, adapter_path).eval().to(
                torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
            )
            tokenizer    = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)

            # ensure tokens
            if tokenizer.eos_token is None:
                tokenizer.eos_token = tokenizer.pad_token or "</s>"

            chat_hist = "\n".join(f"{m['role']}: {m['content']}" for m in st.session_state.history + [{"role":"user","content":prompt}])
            inputs    = tokenizer(chat_hist, return_tensors="pt", truncation=True, max_length=2048).to(model.device)
            outputs   = model.generate(
                **inputs,
                max_new_tokens=6000,
                do_sample=True, temperature=0.7, top_p=0.95,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
            text      = tokenizer.decode(outputs[0], skip_special_tokens=True)
        else:
            text = generate_code_response(
                prompt=prompt,
                model_choice="deepseek-coder-1.3b-instruct",
                hf_token=hf_token,
                mode=mode
            )

    # append & display
    st.session_state.history.append(("You", prompt))
    st.session_state.history.append(("Bot", text))

# â”€â”€ Render history â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for speaker, txt in st.session_state.history:
    st.markdown(f"**{speaker}:**  \n```{txt}```")

# â”€â”€ Reset button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.sidebar.button("Reset History"):
    reset_history()
    st.experimental_rerun()

# â”€â”€ Cleanup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if torch.cuda.is_available():
    torch.cuda.empty_cache()
gc.collect()
